# O Modelo Linear I
## Representação da entrada
Um dataset real - reconhecimento de números escritos à mão
Entrada "bruta" - um grid de 16x16 pixels. Cada pixel é representado por um número real positivo.
No nosso modelo linear, isso dá 257 variáveis a serem ajustadas pelo nosso algoritmo de classificação, o que é muito. Sabemos que não precisamos dos mínimos detalhes da imagem que representa cada dígito, então podemos extrair características de cada imagem para diminuir o nosso número de variáveis.\
No nosso caso, podemos representar cada imagem com apenas duas variáveis: intensidade e simetria.

## Classificação linear - O algoritmo "pocket"
Quando temos dados que não são linearmente separáveis, o nosso algotimo do perceptron não converge, então precisamos limitar o número máximo de iterações. O problema disso é que às vezes, ao ajustar nossa reta (ou hiperplano) de decisão para classificar corretamente um ponto em uma determinada iteração, outros pontos podem passar a ser classificados erroneamente, então o nosso erro na amostra (ou dataset) $E_{\text{in}}$ pode oscilar muito durante a execução, e nada nos garante que a hipótese escolhida (no nosso caso, o vetor peso $w$) na última iteração possível é a hipótese que melhor aproxima a nossa função objetivo. 

Uma simples modificação no nosso algoritmo do perceptron pode mudar isso: ao invés de simplesmente limitarmos o número máximo de iterações e ficar com a hipótese que obtivemos na última iteração, guardamos a hipótese com o menor erro $E_{\text{in}}$ entre todas as testadas e devolvemos essa hipótese no final da execução do algoritmo. No nosso caso, a medida do erro é simplesmente a razão entre pontos mal classificados e pontos totais no dataset.

## Regressão linear
Nos algoritmos de classificação, estamos preocupados em classificar os dados em duas (ou mais) classes. Para isso, nosso algoritmo produz uma saída binária (no caso de duas categorias. Para $n$ categorias, nosso algoritmo produziria saídas $n$-árias). Quando a nossa função objetivo leva a valores reais, dizemos que temos um problema de *regressão*.\
Vamos voltar ao exemplo de aprovação de crédito. Ao invés de apenas fazer uma decisão binária (aprovar ou não o crédito de um aplicante), digamos que o banco queira estabelecer um limite de crédito apropriado para cada cliente.

Este é um problema de aprendizado de regressão. Agora, nosso dataset $\mathcal{D}$ consiste de elementos $(x_1, y_1), \dots, (x_N, y_N)$, onde, para $i \in \{1, \dots, N\}$ $x_i$ representa os dados do aplicante, do mesmo modo do problema de classificação, porém, $y_i$ representa o limite de crédito aprovado para esse cliente. Note que $y_i$ é um número real, ao invés de um valor binário $\pm 1$.

### O algoritmo
O algoritmo de regressão linear é baseado em minimizar o erro quadrático entre $h(\mathrm{x})$ e $y$.\
Assim como no algoritmo do perceptron, nossa hipótese $h$ tem a forma de uma combinação linear das componentes de $\mathrm{x}$:
$$
h(\mathrm{x}) = \sum_{i = 0}^d w_i x_i = \mathrm{w^ T x}
$$
onde $x_0 = 1$

Então, nossa expressão para o erro quadrático médio fica:
$$
\begin{align*}
   E_\text{in}(\textbf{w})
   &= \dfrac{1}{N} \sum_{n=1}^N (\mathrm{w^ T x_n} - y_n)^2\\
   &= \dfrac{1}{N} ||\mathrm{X w - y}||^2
\end{align*}
$${#eq:eqm1}
Onde $||\cdot||$ é a norma euclidiana de um vetor, e
$$
\mathrm{X} = \begin{bmatrix}
   —\mathrm{x_1^T}—\\
   —\mathrm{x_2^T}—\\
   \vdots \\
   —\mathrm{x_\mathit{N}^T}—\\
\end{bmatrix}
\quad \text{e} \quad 
\mathrm{y} = \begin{bmatrix}
   y_1\\
   y_2\\
   \vdots \\
   y_N
\end{bmatrix}
$$
Então podemos escrever @eq:eqm1 como
$$
E_\text{in}(\textbf{w}) = 
\dfrac{1}{N} (\mathrm{w^T X^T X w - 2w^T X^Ty + y^Ty})
$${#eq:eqm2}

O algoritmo de regressão linear é derivado ao minimizar $E_\text{in}(\textbf{w})$ sobre todo o espaço de hipótese. Como a eq @eq:eqm2 implica que $E_\text{in}(\textbf{w})$ é diferenciável, podemos usar o cálculo padrão de matrizes para encontrar $\textbf{w}$ que minimize $E_\text{in}(\textbf{w})$, obtendo a expressão para o seu gradiente e o igualando a zero (ou melhor, ao *vetor* zero), isto é, $\nabla E_\text{in}(\textbf{w}) = \mathbf{0}$. Então obtemos:
$$
\nabla E_\text{in}(\textbf{w}) = \dfrac{2}{N}
(\mathrm{X^TXw - X^Ty}) = \mathbf{0}
$$
o que implica
$$
\mathrm{X^TXw = X^Ty}
$$

Se $\mathrm{X^TX}$ for inversível, então $\mathrm{w = X^\dagger y}$, onde $\mathrm{X^\dagger = (X^TX)^{-1}X^T}$ é o [*pseudo-inverso*](https://en.wikipedia.org/wiki/Generalized_inverse) de $\mathrm{X}$.
