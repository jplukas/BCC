# MAC0315 - Otimizacao Linear
## Funcoes objetivas lineares por partes convexas
### Definicao 1.1: 
(a) Uma funcao $f: \mathfrak{R}^n \mapsto \mathfrak{R}$ eh chamada *convexa* se para todos $x, y \in \mathfrak{R}$, e todo $\lambda \in [0, 1]$, nos temos:
$$
f(\lambda x + (1 - \lambda) y) \leq 
\lambda f(x) + (1 - \lambda) f(y)
$$
(b) Uma funcao $f: \mathfrak{R}^n \mapsto \mathfrak{R}$ eh chamada *concava* se para todos $x, y \in \mathfrak{R}$, e todo $\lambda \in [0, 1]$, nos temos:
$$
f(\lambda x + (1 - \lambda) y) \geq 
\lambda f(x) + (1 - \lambda) f(y)
$$

Note que se $x$ e $y$ forem vetores em $\mathfrak{R}^n$, e se $\lambda$ variar em $[0, 1]$, entao os pontos da forma $\lambda x + (1 - \lambda) y$ pertencem ao segmento de reta que passa por $x$ e $y$.
Se $f$ for linear, a inequacao na parte (a) seria uma equacao. A inequacao portanto significa que, quando nos restringimos ao tal segmento, o grafico da funcao nao eh maior que a funcao linear correspondente. Veja a figura 1.1(a).

Dizemos que um vetor $x$ eh um *minimo local* de $f$ se $f(x) \geq f(y)$ para todo $y$ na vizinhanca de $x$. Tambem dizemos que $x$ eh um *minimo global* de $f$ se $f(x) \geq f(y)$ para todo $y$. Uma funcao convexa nao pode ter minimos locais que nao sejam tambem minimos globais, e essa propriedade eh de grande ajuda para desenvolver algoritmos de otimizacao eficientes.

Seja $c_1, \dots, c_m$ vetores em $\mathfrak{R}^n$, sejam $d_1, \dots, d_m$ escalares e considere a funcao $f: \mathfrak{R} \mapsto \mathfrak{R}$ definida por:
$$
f(x) = \max\limits_{i = 1, \dots, m} (c_i^T x + d_i)
$$
Tal funcao eh convexa, como consequencia do seguinte resultado:

#### Teorema 1.1:
Sejam $f_1, \dots f_m : \mathfrak{R}^n \mapsto \mathfrak{R}$ funcoes convexas. Entao a funcao $f$ definida por $f(x) = \max\limits_{i = 1, \dots, m} f_i(x)$ tambem eh convexa.

##### Prova
Sejam $x, y \in \mathfrak{R}^n$ e seja $\lambda \in [0, 1]$. Temos:
$$
\begin{align*}
f(\lambda x + (1 - \lambda )y) &=
\max\limits_{i = 1, \dots, m} f_i(\lambda x + (1 - \lambda )y) \\
&\leq
\max\limits_{i = 1, \dots, m}
\lambda f_i(x) + (1 - \lambda ) f_i(y) \\
&\leq
\max\limits_{i = 1, \dots, m} \lambda f_i(x) + 
\max\limits_{i = 1, \dots, m} (1 - \lambda ) f_i(y) \\
&= \lambda f(x) + (1 - \lambda) f(y)
\end{align*}
$$

Uma funcao da forma $\max\limits_{i = 1, \dots, m}(c_i^T \mathrm{x} + d_i)$ eh chamada de *funcao convexa linear por partes*. Como ilustrado na figura 1.2(b), uma funcao convexa linear por partes pode ser usada para aproximar uma funcao convexa generalizada. \
Agora consideremos uma generalizacao de programacao linear, onde a funcao objetivo eh linear por partes e convexa, ao inves de linear:
$$
\begin{split}
\text{minimize } &\max\limits_{i = 1, \dots, m}(c_i^T \mathrm{x} + d_i) \\
\text{sujeito a }& Ax \geq b
\end{split}
$$ 

Note que $\max\limits_{i = 1, \dots, m}(c_i^T x + d_i)$ eh igual ao menor numero $z$ que satisfaz $z \geq c_i^T x + d_i$ para todo $i$. Por essa razao, o problema de otimizacao linear sob consideracao eh equivalente ao seguinte problema:
$$
\begin{align*}
\text{minimize }  z \\
\text{sujeito a } z &\geq c_i^T x + d_i, \quad i = 1, \dots, m \\
Ax &\geq b
\end{align*}
$$
